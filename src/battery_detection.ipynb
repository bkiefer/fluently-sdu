{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO, settings\n",
    "settings.update({\"runs_dir\": \"../runs\", \"weights_dir\": \"../weights\", \"datasets_dir\": \"../data/datasets\"})\n",
    "YOLO_try = False\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import neighbors\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as functional\n",
    "import copy\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import Normalize\n",
    "import open3d as o3d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "import time\n",
    "from matplotlib.lines import Line2D\n",
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1530, 1369])\n",
      "torch.Size([3, 1530, 1369])\n",
      "torch.Size([3, 1530, 1369])\n",
      "torch.Size([3, 1530, 1369])\n"
     ]
    }
   ],
   "source": [
    "class CellDatabase(Dataset):\n",
    "    def __init__(self, folder_path, jpg_shape=(1530, 1369), gauss_p=0.0, s_and_p_p=0.0, speckle_p=0.0):\n",
    "        self.xs = []\n",
    "        self.ys = []\n",
    "        if not os.path.exists(folder_path):\n",
    "            raise Exception(\"Folder not found, dataset not created!\")\n",
    "        subdirs = os.listdir(folder_path)\n",
    "        if not ('annotations' in subdirs and 'images' in subdirs):\n",
    "            raise Exception(\"Folder not in right standard (YOLO), dataset not created!\")\n",
    "        annotations = (sorted(os.listdir(os.path.join(folder_path, 'annotations'))))\n",
    "        images = (sorted( os.listdir(os.path.join(folder_path, 'images'))))\n",
    "        for img_path, ann_path in zip(images, annotations):\n",
    "            x = (self.padding_img(torchvision.io.read_image(os.path.join(folder_path, 'images', img_path)), jpg_shape, 0))\n",
    "            x = x.float()\n",
    "            with open(os.path.join(folder_path, 'annotations', ann_path), \"r\") as file: \n",
    "                labels, boxes = [], []\n",
    "                for line in file:\n",
    "                    content = line.split(' ')\n",
    "                    labels.append(int(content[0]))\n",
    "                    centre_x, centre_y, width, height = [float(element) for element in content[1:]]\n",
    "                    x_min = centre_x - (width / 2)\n",
    "                    y_min = centre_y - (height / 2)\n",
    "                    x_max = centre_x + (width / 2)\n",
    "                    y_max = centre_y + (height / 2)\n",
    "                    boxes.append([x_min, y_min, x_max, y_max])\n",
    "            y = {'labels': (torch.tensor(labels, dtype=torch.int64)), 'boxes': torch.tensor(boxes, dtype=torch.float32)}\n",
    "            self.xs.append(x)\n",
    "            self.ys.append(y)\n",
    "\n",
    "        # mean_val = torch.stack(self.xs).mean()\n",
    "        # std_val = torch.stack(self.xs).std()\n",
    "        # min_val = torch.stack(self.xs).min()\n",
    "        # max_val = torch.stack(self.xs).max()\n",
    "\n",
    "        # if np.random.rand() < gauss_p:\n",
    "        #     noisy = self.add_gaussian_noise(x[0], mean_val, std_val).unsqueeze(0)\n",
    "        #     noisy = (noisy - min_val) / (max_val - min_val + 1e-10)\n",
    "        #     self.xs.append(noisy)\n",
    "        #     self.ys.append(y)\n",
    "        # if np.random.rand() < s_and_p_p:\n",
    "        #     noisy = self.add_salt_pepper_noise(x[0]).unsqueeze(0)\n",
    "        #     noisy = (noisy - min_val) / (max_val - min_val + 1e-10)\n",
    "        #     self.xs.append(noisy)\n",
    "        #     self.ys.append(y)\n",
    "        # if np.random.rand() < speckle_p:\n",
    "        #     noisy = self.add_speckle_noise(x[0]).unsqueeze(0)\n",
    "        #     noisy = (noisy - min_val) / (max_val - min_val + 1e-10)\n",
    "        #     self.xs.append(noisy)\n",
    "        #     self.ys.append(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        y = self.ys[idx]\n",
    "        x = self.xs[idx]\n",
    "        return x, y\n",
    "\n",
    "    def show_x(self, idx, annotate_ys=True):\n",
    "        if annotate_ys:\n",
    "            self.mark_bb_on_img(self.xs[idx], self.ys[idx])\n",
    "        else:\n",
    "            plt.imshow(self.xs[idx].numpy().transpose(1, 2, 0))\n",
    "        \n",
    "    def print_y(self, idx):\n",
    "        print(df.ys[idx])\n",
    "    \n",
    "    @staticmethod\n",
    "    def padding_img(img, desired_shape, padding_value):\n",
    "        padded_channels = []\n",
    "        pad_height_top = (desired_shape[0] - img.shape[1]) // 2\n",
    "        pad_height_bottom = desired_shape[0] - img.shape[1] - pad_height_top\n",
    "        pad_width_left = (desired_shape[1] - img.shape[2]) // 2\n",
    "        pad_width_right = desired_shape[1] - img.shape[2] - pad_width_left\n",
    "        for channel in img:\n",
    "            padded_channel = torch.nn.functional.pad(channel, (pad_width_left, pad_width_right, pad_height_top, pad_height_bottom), value=padding_value)\n",
    "            padded_channels.append(padded_channel)\n",
    "        padded_img = torch.stack(padded_channels)\n",
    "        return padded_img\n",
    "\n",
    "    @staticmethod\n",
    "    def mark_bb_on_img(x, y):\n",
    "        plt.figure(figsize=(10,10))\n",
    "        colors = ['red']\n",
    "        legend_handle = []\n",
    "        for i, clr  in enumerate(colors):\n",
    "            legend_handle.append(Line2D([0], [0], color=clr, marker='s', label=str(i), markersize=5, linestyle='none'))\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        for label, box in zip(y['labels'], y['boxes']):\n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            ax.add_patch(patches.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, linewidth=2, edgecolor=colors[label], facecolor='none'))\n",
    "        plt.legend(handles=legend_handle, bbox_to_anchor=(1, 1))\n",
    "        plt.imshow(x.numpy().transpose(1, 2, 0))\n",
    "          \n",
    "    @staticmethod\n",
    "    def normalize_img(img, min_value, max_value):\n",
    "        return (img - min_value) / (max_value - min_value + 1e-10)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_gaussian_noise(image, mean, std):\n",
    "        return image + np.random.normal(mean, std, image.shape)\n",
    "    \n",
    "    @staticmethod\n",
    "    def add_salt_pepper_noise(image, prob=0.01):\n",
    "        noisy = copy.deepcopy(image)\n",
    "        # noisy = np.array(noisy)\n",
    "        noise_prob = np.random.rand(noisy.shape[0], noisy.shape[1])\n",
    "        noisy[noise_prob < prob / 2] = noisy.min()  # Pepper (black)\n",
    "        noisy[noise_prob > 1 - prob / 2] = noisy.max()  # Salt (white)\n",
    "        return noisy\n",
    "\n",
    "    @staticmethod\n",
    "    def add_speckle_noise(image, std=0.2):\n",
    "        img = copy.deepcopy(image)\n",
    "        # img = np.array(img)\n",
    "        noise = np.random.normal(0, std, img.shape)\n",
    "        return img + img * noise\n",
    "\n",
    "df = CellDatabase(\"../data/cells_dataset\")\n",
    "data_loader = DataLoader(df, batch_size=1, num_workers=4, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "i = 0\n",
    "for x, y in data_loader:\n",
    "    print(x[0].shape)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO Try (on hold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if YOLO_try:\n",
    "    # Create a new YOLO model from scratch\n",
    "    # model = YOLO(\"yolo11n.yaml\")\n",
    "\n",
    "    # Load a pretrained YOLO model (recommended for training)\n",
    "    model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "\n",
    "    # Train the model using the 'coco8.yaml' dataset for 3 epochs\n",
    "    results = model.train(data=\"coco8.yaml\", epochs=3)\n",
    "\n",
    "    img = cv2.imread(\"../data/NMC21700-from-top.jpg\")\n",
    "    print(img.shape)\n",
    "    results = model.predict(source=img, save=True)\n",
    "    print(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster R-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained Faster R-CNN model with a ResNet-50 backbone\n",
    "model = fasterrcnn_resnet50_fpn(weights=None)\n",
    "\n",
    "# Number of classes (your dataset classes + 1 for background)\n",
    "num_classes = 2  # For example, 2 classes + background\n",
    "\n",
    "# Get the number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "# Replace the head of the model with a new one (for the number of classes in your dataset)\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50, Loss: 8.200415313243866\n",
      "Epoch: 2/50, Loss: 4.67921656370163\n",
      "Epoch: 3/50, Loss: 2.919365644454956\n",
      "Epoch: 4/50, Loss: 2.051829904317856\n",
      "Epoch: 5/50, Loss: 1.9831268787384033\n",
      "Epoch: 6/50, Loss: 1.9270646274089813\n",
      "Epoch: 7/50, Loss: 1.8880155682563782\n",
      "Epoch: 8/50, Loss: 1.8682940602302551\n",
      "Epoch: 9/50, Loss: 1.8844190835952759\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[221], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m loss_dict\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Move model to GPU if available\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Set up the optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001, momentum=0.0, weight_decay=0.0005)\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "# Train the model\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "   # Training loop\n",
    "    for images, targets in data_loader:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        # image = image.to(device)\n",
    "        # target = target.to(device)\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        # Backward pass\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += losses.item()\n",
    "\n",
    "    # Update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    print(f'Epoch: {epoch + 1}/{num_epochs}, Loss: {train_loss / len(data_loader)}')\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 100\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Test on a new image\n",
    "id = 0\n",
    "good = None\n",
    "with torch.no_grad():\n",
    "    for img, _ in data_loader:\n",
    "        img = df.xs[id].to(device)\n",
    "        predictions = model([img])\n",
    "        if predictions[0]['boxes'].shape[0] > 0:\n",
    "            good = predictions\n",
    "            break\n",
    "        id += 1\n",
    "if good is None:\n",
    "    print(\"No hit\")\n",
    "else:\n",
    "    print(id, len(predictions[0]['boxes']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show_x(0)\n",
    "df.mark_bb_on_img()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
